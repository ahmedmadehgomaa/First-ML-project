{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f450a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most Important \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## other\n",
    "import missingno as msno\n",
    "import os\n",
    "\n",
    "## Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "## Preprocessing \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# from sklearn_features.transformers import DataFrameSelector\n",
    "\n",
    "## Models\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bedbe2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matrix\\FirstMLproject.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matrix/FirstMLproject.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## Read the csv file using pandas\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matrix/FirstMLproject.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(os\u001b[39m.\u001b[39mgetcwd(), \u001b[39m'\u001b[39m\u001b[39mhousing.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matrix/FirstMLproject.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_housing \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matrix/FirstMLproject.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m## show the head of the DF\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matrix/FirstMLproject.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df_housing\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## Read the csv file using pandas\n",
    "file_path = os.path.join(os.getcwd(), 'housing.csv')\n",
    "df_housing = pd.read_csv(file_path)\n",
    "\n",
    "## show the head of the DF\n",
    "df_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7a6dc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## check the info (nulls and datatypes)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf_housing\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_housing' is not defined"
     ]
    }
   ],
   "source": [
    "## check the info (nulls and datatypes)\n",
    "df_housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff52d7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## some statistics of the DF --> target column is (median_house_value)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf_housing\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_housing' is not defined"
     ]
    }
   ],
   "source": [
    "## some statistics of the DF --> target column is (median_house_value)\n",
    "df_housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the nulls\n",
    "df_housing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e79ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the Distribution of (ocean_proximity) Feature in Dataset\n",
    "ocean_values = df_housing['ocean_proximity'].value_counts()\n",
    "print('categories of (ocean_proximity) --- \\n', ocean_values)\n",
    "print('**'*40)\n",
    "\n",
    "## plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='ocean_proximity', data=df_housing, order=ocean_values.index)  ## ordered\n",
    "plt.title('CountPlot of (ocean_proximity) Feature in Dataset', fontsize=14, c='k')\n",
    "plt.xlabel('ocean_proximity', fontsize=14, c='k')\n",
    "plt.ylabel('Counts', fontsize=14, c='k')\n",
    "\n",
    "## showing the percenatge\n",
    "for i in range(ocean_values.shape[0]):\n",
    "    count = ocean_values[i]\n",
    "    strt='{:0.2f}%'.format(100*count / df_housing.shape[0])\n",
    "    plt.text(i, count+100, strt, ha='center', color='blue', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram Distribution of Numerical Variables in Dataset\n",
    "df_housing.hist(bins=30, figsize=(20,10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scatte plot with important features (median_income) with the target (median_house_value)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_housing['median_income'], df_housing['median_house_value'], alpha=0.4)\n",
    "plt.title('Distribution of median income with house value', fontsize=14, c='k')\n",
    "plt.xlabel('Median Income', fontsize=14, c='k')\n",
    "plt.ylabel('Median House Value', fontsize=14, c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016cb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try for (median_house_value) to convert it to chunks (bins)\n",
    "## Cut this numerical feature to categorical for its values to be in range will be in the same category\n",
    "## In the following example -- any value between range (0-100k) will be the same category, I name it (0-100k)\n",
    "house_value_bins = pd.cut(x=df_housing['median_house_value'], \n",
    "                          bins=[0, 100000, 200000, 300000, 400000, 500000, np.inf], \n",
    "                          labels=['0-100k', '100k-200k', '300k-400k', '400k-500k', '500k-600k', '600k<'])\n",
    "\n",
    "## countplot for the above chunks\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=house_value_bins)\n",
    "plt.title('CountPlot of House Value Bins in Dataset', fontsize=14, c='k')\n",
    "plt.xlabel('House Value Bins', fontsize=14, c='k')\n",
    "plt.ylabel('Counts', fontsize=14, c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try the same above code for (median_income), I think it is very important feature\n",
    "income_bins = pd.cut(x=df_housing['median_income'], \n",
    "                          bins=[0, 1.5, 3, 4.5, 6, np.inf],\n",
    "                          labels=['0-1.5', '1.5-3', '3-4.5', '4.5-6', '6<'])\n",
    "\n",
    "## countplot for the above chunks\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=income_bins)\n",
    "plt.title('CountPlot of Income Bins in Dataset', fontsize=14, c='k')\n",
    "plt.xlabel('Income Bins', fontsize=14, c='k')\n",
    "plt.ylabel('Counts', fontsize=14, c='k')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter plot with (median_income) and (median_house_value) at each (ocean_proximity) \n",
    "sns.relplot(x='median_income', y='median_house_value', data=df_housing, kind='scatter', \n",
    "            col='ocean_proximity', col_wrap=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c339dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the Boxplot of the (median_income) at different categories of (ocean_proximity)\n",
    "sns.catplot(y='median_income', x='ocean_proximity', data=df_housing, kind='box', height=5, aspect=1.4)\n",
    "plt.title('Boxplot of Median Income at different ocean_proximity Categories', fontsize=14, c='k')\n",
    "plt.xlabel('Ocean Proximity', fontsize=14, c='k')\n",
    "plt.ylabel('Median Income', fontsize=14, c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd013d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the Boxplot of the (median_house_value) at different categories of (ocean_proximity)\n",
    "sns.catplot(y='median_house_value', x='ocean_proximity', data=df_housing, kind='box', height=5, aspect=1.4)\n",
    "plt.title('Boxplot of Median House Value at different ocean_proximity Categories', fontsize=14, c='k')\n",
    "plt.xlabel('Ocean Proximity', fontsize=14, c='k')\n",
    "plt.ylabel('Median House Value', fontsize=14, c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7429053",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter plot for (longitued & latitued), and add population as the size of the point and the color as (house_value)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sc = plt.scatter(df_housing['longitude'], df_housing['latitude'], s=df_housing['population']/100,\n",
    "                   alpha=0.4, c=df_housing['median_house_value'], cmap=plt.get_cmap('jet'), label='population')\n",
    "plt.colorbar(sc)\n",
    "plt.xlabel('Longitude', fontsize=14, c='k')\n",
    "plt.ylabel('Latitude', fontsize=14, c='k')\n",
    "plt.title('Longitude vs. Latitude', fontsize=14, c='k')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0697b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlatio (pearson correlation)\n",
    "df_housing.corr()['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can show this as heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df_housing.corr(), annot=True, cmap='Blues')  ## symmetric matrix\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18174fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to make some Feature Engineering --> Feature Extraction --> Add the new column to the main DF\n",
    "df_housing['rooms_per_household'] = df_housing['total_rooms'] / df_housing['households']\n",
    "df_housing['bedroms_per_rooms'] = df_housing['total_bedrooms'] / df_housing['total_rooms']\n",
    "df_housing['population_per_household'] = df_housing['population'] / df_housing['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919be692",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the correlation again\n",
    "df_housing.corr()['median_house_value'].sort_values(ascending=False)  ## some progress for out new features (very good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49569306",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the Whole dataset to features and target\n",
    "X = df_housing.drop(columns=['median_house_value'], axis=1)  ## features\n",
    "y = df_housing['median_house_value']  ## target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random split the dataset to two sets (train_set, valid_set, test_set)\n",
    "## Firstly split to (train_full_set, test_set) then split (train_full_set) to (train_set, valid_set)\n",
    "## Firstly\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.15, random_state=123, shuffle=True)\n",
    "## secondly\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=125, shuffle=True)\n",
    "\n",
    "## check shape\n",
    "print('X_train shape -- ', X_train.shape)\n",
    "print('y_train shape -- ', y_train.shape)\n",
    "print('X_valid shape -- ', X_valid.shape)\n",
    "print('y_valid shape -- ', y_valid.shape)\n",
    "print('X_test shape -- ', X_test.shape)\n",
    "print('y_test shape -- ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separete the columns according to type (numerical or categorical)\n",
    "num_cols = [col for col in  X_train.columns \n",
    "             if X_train[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "categ_cols = [col for col in  X_train.columns \n",
    "                if X_train[col].dtype not in ['float64', 'int64']]\n",
    "\n",
    "print('Numerical Columns : \\n', num_cols)\n",
    "print('**'*30)\n",
    "print('Categorical Columns : \\n', categ_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c446015",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I choose to impute the nulls with median --> using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')  ## define an instance\n",
    "\n",
    "## fit and transform to training data (only Numerical)\n",
    "X_train_filled = imputer.fit_transform(X_train[num_cols])\n",
    "\n",
    "## transform valid and test datasets\n",
    "X_valid_filled = imputer.transform(X_valid[num_cols])\n",
    "X_test_filled = imputer.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3faa89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using StandardScaler for each Feature to be (mean=0, std=1)\n",
    "## This techniques helps for better training and to converge faster and put all features in the same scale\n",
    "scaler = StandardScaler()  ## define an instance\n",
    "\n",
    "## fit and transform to training data (only Numerical)\n",
    "X_train_scaled = scaler.fit_transform(X_train_filled)\n",
    "\n",
    "## transform valid and test datasets\n",
    "X_valid_scaled = scaler.transform(X_valid_filled)\n",
    "X_test_scaled = scaler.transform(X_test_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00da85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a pipeline for numerical variables\n",
    "num_pipline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "## deal with (num_pipline) as an instance -- fit and transform to train dataset and transform only to other datasets\n",
    "X_train_num = num_pipline.fit_transform(X_train[num_cols])\n",
    "X_valid_num = num_pipline.transform(X_valid[num_cols])\n",
    "X_test_num = num_pipline.transform(X_test[num_cols])  ## much easier and much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use here for (ocean_proximity) --> use LabelEncoder (Try to use OHE)\n",
    "lbl_encoder = LabelEncoder()    ## define an instance\n",
    "X_train_encoded = lbl_encoder.fit_transform(X_train[categ_cols].values[:, 0])  ## for train\n",
    "\n",
    "X_valid_encoded = lbl_encoder.transform(X_valid[categ_cols].values[:, 0])\n",
    "X_test_encoded = lbl_encoder.transform(X_test[categ_cols].values[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define pipeline for categorical columns --> use OneHotEncoder = OHE\n",
    "categ_pipeline = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('OHE', OneHotEncoder(sparse=False))])\n",
    "\n",
    "\n",
    "## deal with (categ_pipeline) as an instance -- fit and transform to train dataset and transform only to other datasets\n",
    "X_train_categ = categ_pipeline.fit_transform(X_train[categ_cols])\n",
    "X_valid_categ = categ_pipeline.transform(X_valid[categ_cols])\n",
    "X_test_categ = categ_pipeline.transform(X_test[categ_cols])  ## much easier and much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d404b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can get much much easier like the following\n",
    "## numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "                    ('selector', DataFrameSelector(num_cols)),    ## select only these columns\n",
    "                    ('imputer', SimpleImputer(strategy='median')),\n",
    "                    ('scaler', StandardScaler())])\n",
    "\n",
    "## categorical pipeline\n",
    "categ_pipeline = Pipeline(steps=[\n",
    "            ('selector', DataFrameSelector(categ_cols)),    ## select only these columns\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('OHE', OneHotEncoder(sparse=False))])\n",
    "\n",
    "## concatenate both two pipelines\n",
    "total_pipeline = FeatureUnion(transformer_list=[\n",
    "                                ('num_pip', num_pipeline),\n",
    "                                ('categ_pipeline', categ_pipeline)])\n",
    "\n",
    "## deal with (total_pipeline) as an instance -- fit and transform to train dataset and transform only to other datasets\n",
    "X_train_final = total_pipeline.fit_transform(X_train)\n",
    "X_valid_final = total_pipeline.transform(X_valid)\n",
    "X_test_final = total_pipeline.transform(X_test)                 ### Every thing is processed :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_final, y_train)  ## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction\n",
    "y_pred_train_lin = lin_reg.predict(X_train_final)  ## predict the training data\n",
    "y_pred_valid_lin = lin_reg.predict(X_valid_final)  ## predict the training data\n",
    "\n",
    "## RMSE\n",
    "rmse_train_lin = (mean_squared_error(y_train, y_pred_train_lin))**0.5\n",
    "rmse_valid_lin = (mean_squared_error(y_valid, y_pred_valid_lin))**0.5\n",
    "\n",
    "## R2 score\n",
    "r2_train_lin = r2_score(y_train, y_pred_train_lin)\n",
    "r2_valid_lin = r2_score(y_valid, y_pred_valid_lin)\n",
    "\n",
    "\n",
    "print(f'RMSE for training set using LinearRegression -- {rmse_train_lin:.3f}')\n",
    "print(f'RMSE for validating set using LinearRegression -- {rmse_valid_lin:.3f}')   \n",
    "print('**'*40)\n",
    "print(f'R2 Score for training set using LinearRegression -- {r2_train_lin:.3f}')\n",
    "print(f'R2 Score for validating set using LinearRegression -- {r2_valid_lin:.3f}')      ## not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2844e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model --> ## initialize some random Hyperparameter --> we will tune later\n",
    "sgd_reg = SGDRegressor(penalty=None, eta0=0.001, shuffle=True, max_iter=10000)  \n",
    "sgd_reg.fit(X_train_final, y_train)  ## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction\n",
    "y_pred_train_sgd = sgd_reg.predict(X_train_final)  ## predict the training data\n",
    "y_pred_valid_sgd = sgd_reg.predict(X_valid_final)  ## predict the training data\n",
    "\n",
    "## RMSE\n",
    "rmse_train_sgd = (mean_squared_error(y_train, y_pred_train_sgd))**0.5\n",
    "rmse_valid_sgd = (mean_squared_error(y_valid, y_pred_valid_sgd))**0.5\n",
    "\n",
    "## R2 score\n",
    "r2_train_sgd = r2_score(y_train, y_pred_train_sgd)\n",
    "r2_valid_sgd = r2_score(y_valid, y_pred_valid_sgd)\n",
    "\n",
    "\n",
    "print(f'RMSE for training set using SGDRegression -- {rmse_train_sgd:.3f}')\n",
    "print(f'RMSE for validating set using SGDRegression -- {rmse_valid_sgd:.3f}')   \n",
    "print('**'*40)\n",
    "print(f'R2 Score for training set using SGDRegression -- {r2_train_sgd:.3f}')\n",
    "print(f'R2 Score for validating set using SGDRegression -- {r2_valid_sgd:.3f}')         ## not bad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "beb78d989005f92972e9d055f81ccee39b3d15a8c7d93959df4751f2a0ded231"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
